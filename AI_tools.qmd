---
title: "AI tools: Tips und Tricks für LLMs"
author: "Hannah Metzler"
date: "2. April 2024"
format: 
  revealjs: 
    transition: 'slide'
    ## Defines the theme of the presentation, both version and style
    theme: [default, AI_tools_files/libs/custom.scss]
#editor
editor: visual
---

## Was sind LLMs eigentlich?

- Large Language Models:
- Fortschrittliche Sprachmodelle, die riesige Mengen an Textdaten verarbeiten
- Trainiert mit Textdaten aus dem Internet (Wikipedia, Medien, Social Media...), Literatur, Videos, Filme Podcasts, Medien...
- Statistische Wort und Satz Vorhersagen, abgeleitet aus riesigen Datenmengen
- können ausführliche und kontextreiche Antworten liefern

## Google vs. LLMs: häufige Fehler

- Suchmaschine: 
  - wenige kurze Keywords
  - Antwort: viele einzelne Ergebnisse

- LLM:
  - Viel Details, Informationen und Kontext geben
  - Sprechen wie mit einer Person
  - Feedback geben & Dialog führen
  - Antwort ist zusammenfassend
  

## Anwendungsbeispiele

- Alltägliche Anfragen: 
  - Veranstaltungen, Freizeitaktivitäten, Komplimente.
  - Rezeptvorschläge, Einkaufslisten.
- Forschung und Studium: Zusammenfassung langer Berichte.

## Emotionale Prompts

- Emotionale Ausdrücke können zu besseren Ergebnissen führen
- Training des Modells auf menschlicher Sprache
- Statistik: wahrscheinlich besser oder schlechter wenn ich sage es ist wichtig?
- Modell verhält sich wie Menschen
- Kann Sorgfalt der Modelle bei der Antwortfindung beeinflussen.

## Bedeutung für die Zukunft

- Die Rolle von KI-Technologien wie ChatGPT wird weiter zunehmen
- Wir lernen laufen wie wir sie am besten verwenden können
- Täglich benützen und kreativ sein! 

# Anwendungen

## Anwendungen im Alltag

- **Rezepte**: Vorschläge basierend auf vorhandenen Zutaten.
- **Reiseplanung**: Individuelle Empfehlungen für Städtereisen, basierend auf persönlichen Präferenzen.
- **Einkaufslisten**: Schitzeln und Gulasch für 4 Personen

## Anwendungen bei der Arbeit

- **Feedback zu Präsentationen**: Analyse und Verbesserungsvorschläge für Folien.
- **Hyper-Personalisierung**: Analyse von LinkedIn-Profilen für zielgerichtete Verkaufsstrategien.

### Ausprobieren!

- **Akademische Forschung**: 
  - Systematische Literaturübersichten, Zusammenfassungen von Fachartikeln.
  - Feedback für Präsentationen
  - Struktur eine Präsentation erstellen lassen
  - Programmieren
  - Texte überarbeiten
  - erster Entwurf eines Textes schreiben anhand von Notizen
  - im eigenen Schreibstil schreiben: Paper reinladen
  - eigenes Wissen zusammenfassen
- **Textgenerierung**: Erstellung von Inhalten, Übersetzungen, Zusammenfassungen.


## Tipps und Tricks

- **Baseprompts**: Nach einer erfolgreichen Interaktion einen Baseprompt in JSON formatieren lassen für zukünftige Anfragen.
- Antwort eines LLMs mit anderem LMM checken: Stimmt das?
- Modell nachfragen: überprüfe das nochmal, ist das so korrekt? Kannst du mir Quellen dazu geben?
- 80% mit ChatGPT, dann selbst überprüfen - erster Entwurf

## Halluzinationen

- Viel weniger Problem als zu Beginn

## Aktuelle Top-Modelle

- **Gemini**
- **ChatGPT 4**
- **Claude**

## Stärken verschiedener Modelle

- **Claude**: längere Texte, besitzt ein besseres Gedächtnis (500 Seiten) (Context window)
- **Perplexity AI**: Funktioniert wie eine Suchmaschine, bietet direkte Antworten aus dem Web mit wissenschaftlichen Quellen
- **Bard**: Von Google, vielseitig einsetzbar.

## Chat GPT 4

- ChatGPT 4 ist viel besser als die kostenlosen Version
- Handyapp mit Mikrofon Funktion: Unterhaltung
- Multimodales Arbeiten
- Bilder, Screenshots reinladen
- Bilder kreiren

# Fortgeschrittene Techniken

- **Custom GPT**: Erstellung eines angepassten Modells für spezifische Anforderungen oder zur Simulation eines digitalen Zwillings.
- System/Base prompts

# Advanced Notes

## Joao talk

Evaluating LLMs - what is trustworthy
- Chatbot Arena
- r/LocalLamma comments: best place to learn about this stuff
    - a Starter guide for playing with your own local AI
    - The llama hitchiking guide to local LLMs
Other good sources: Medium, Twitter

Mixture of experts - very promising: another architecture, e.g. Mixtral: trained with 8 mistrals, at each token it is chosing 2 different experts, trained on high dimensional separations of the data

:::{.notes}
Best open source model: Mixtral
LoRa: Low Rank Adaptation: https://arxiv.org/abs/2106.09685
tps: tokens per second
:::
